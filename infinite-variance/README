♾️ Infinite Variance in Off-Policy Monte Carlo Methods 🎲

This project delves into the challenges of estimating value functions in reinforcement learning, particularly focusing on the infinite variance problem encountered in off-policy Monte Carlo methods using ordinary importance sampling.

📚 Background

In reinforcement learning, off-policy methods allow learning about one policy while following another. However, using ordinary importance sampling in such scenarios can lead to estimates with infinite variance, making them unreliable. This project aims to illustrate this phenomenon through simulations and visualizations.

🗂️ Project Structure
	•	src/: Contains the core implementation of the simulation.
	•	notebooks/: Jupyter notebooks for interactive exploration and visualization.
	•	generated_images/: Stores images generated from the simulations.
	•	book_images/: Contains images referenced from literature for comparison.