# ðŸŽ² Random Walk â€” n-step TD Experiments

This subproject implements and compares **n-step Temporal Difference (n-step TD)** learning on the classic **Random Walk** problem â€” a simple, controlled environment that demonstrates the relationship between Monte Carlo (infinite-step) and one-step TD learning. It provides clean code, experiments, and visualizations to illustrate how the number of backup steps `n` affects **bias**, **variance**, and **learning speed**.

---

## ðŸ§­ Overview

The **Random Walk** task is a benchmark from Sutton & Bartoâ€™s *Reinforcement Learning: An Introduction*.  
An agent starts in the middle of a 1D chain of states and moves left or right randomly until it reaches a terminal state. Rewards occur only at the terminal ends (typically +1 on the right, 0 on the left).  

This setup is simple yet powerful â€” it allows clear, quantitative comparisons of TD learning methods and how multi-step targets influence convergence and stability.

---

## âœ… What this project explores

- Implementation of **n-step TD** learning (for any `n â‰¥ 1`)
- Empirical comparison of **TD(0)**, **n-step TD**, and **Monte Carlo** (infinite-step) targets  
- Study of **bias vs variance** trade-offs as a function of `n`
- Sensitivity to **learning rate (Î±)** and **discount factor (Î³)**
- Aggregated results across random seeds for statistical reliability

---
