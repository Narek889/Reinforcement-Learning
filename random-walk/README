Random Walk – Reinforcement Learning Experiments 🔁🧠

This project implements and compares various reinforcement learning algorithms on the classic Random Walk problem,
as introduced in Sutton & Barto’s Reinforcement Learning: An Introduction.

⸻

Overview 🗺️

The Random Walk environment is a simple linear chain of states with terminal states at both ends. 
An agent starts in the center and moves left or right with equal probability until it reaches a terminal state. 
The goal is to predict the expected return (value) of each non-terminal state.

⸻

Implemented Algorithms ⚙️
	•	Monte Carlo (MC) Prediction 🎲: Estimates value functions based on averaging returns following each visit to a state.
	•	Temporal Difference (TD(0)) ⏱️: Updates value estimates based on the difference between successive predictions.
	•	n-step TD ➡️➡️: Generalizes TD(0) by updating estimates based on n-step returns.
	•	TD(λ) 🧮: Combines MC and TD methods using eligibility traces and a decay parameter λ.