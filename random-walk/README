Random Walk â€“ Reinforcement Learning Experiments ğŸ”ğŸ§ 

This project implements and compares various reinforcement learning algorithms on the classic Random Walk problem,
as introduced in Sutton & Bartoâ€™s Reinforcement Learning: An Introduction.

â¸»

Overview ğŸ—ºï¸

The Random Walk environment is a simple linear chain of states with terminal states at both ends. 
An agent starts in the center and moves left or right with equal probability until it reaches a terminal state. 
The goal is to predict the expected return (value) of each non-terminal state.

â¸»

Implemented Algorithms âš™ï¸
	â€¢	Monte Carlo (MC) Prediction ğŸ²: Estimates value functions based on averaging returns following each visit to a state.
	â€¢	Temporal Difference (TD(0)) â±ï¸: Updates value estimates based on the difference between successive predictions.
	â€¢	n-step TD â¡ï¸â¡ï¸: Generalizes TD(0) by updating estimates based on n-step returns.
	â€¢	TD(Î») ğŸ§®: Combines MC and TD methods using eligibility traces and a decay parameter Î».