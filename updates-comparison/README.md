# üîÅ Updates Comparison ‚Äî Reinforcement Learning Algorithms

This subproject compares different update rules used in tabular reinforcement-learning algorithms. It includes implementations, experiments, visualizations, and short notebooks that demonstrate *how* and *why* different temporal-difference updates (e.g., SARSA, Q-learning, Expected SARSA, and other variants) behave differently on classic benchmark environments.

---

## üß≠ Project Goal

The main objective of this folder is to **implement, test, and compare** common value-update rules in reinforcement learning. The comparison focuses on:

- Convergence speed
- Variance and stability of learning
- Sensitivity to hyperparameters (learning rate, Œµ in Œµ-greedy)
- On-policy vs off-policy differences
- Effect of target estimation (max vs expected vs sampled)

These experiments help highlight practical tradeoffs when choosing an update rule for an RL problem.

---
